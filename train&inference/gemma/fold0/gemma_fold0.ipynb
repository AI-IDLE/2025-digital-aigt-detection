{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQqi8DkulsPB"
      },
      "source": [
        "# 라이브러리 설치 및 임포트, 시드 고정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\2025digital\\2025-digital-aigt-detection\\train&inference\\gemma\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "cd ../../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrIxC-vWQL0p",
        "outputId": "81c71c09-0b61-4de5-929a-63617d555824"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu124\n",
            "Requirement already satisfied: torch==2.6.0+cu124 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from -r ./requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers==4.53.2 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from -r ./requirements.txt (line 2)) (4.53.2)\n",
            "Requirement already satisfied: datasets==4.0.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from -r ./requirements.txt (line 3)) (4.0.0)\n",
            "Requirement already satisfied: peft==0.16.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from -r ./requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: accelerate==1.8.1 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from -r ./requirements.txt (line 5)) (1.8.1)\n",
            "Requirement already satisfied: bitsandbytes==0.46.1 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from -r ./requirements.txt (line 6)) (0.46.1)\n",
            "Requirement already satisfied: fsspec==2025.3.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from -r ./requirements.txt (line 7)) (2025.3.0)\n",
            "Requirement already satisfied: numpy==2.0.2 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from -r ./requirements.txt (line 8)) (2.0.2)\n",
            "Requirement already satisfied: pandas==2.2.2 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from -r ./requirements.txt (line 9)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from -r ./requirements.txt (line 10)) (1.6.1)\n",
            "Requirement already satisfied: scipy==1.15.3 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from -r ./requirements.txt (line 11)) (1.15.3)\n",
            "Requirement already satisfied: tqdm==4.67.1 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from -r ./requirements.txt (line 12)) (4.67.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from torch==2.6.0+cu124->-r ./requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from torch==2.6.0+cu124->-r ./requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from torch==2.6.0+cu124->-r ./requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from torch==2.6.0+cu124->-r ./requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from torch==2.6.0+cu124->-r ./requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from transformers==4.53.2->-r ./requirements.txt (line 2)) (0.33.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from transformers==4.53.2->-r ./requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from transformers==4.53.2->-r ./requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from transformers==4.53.2->-r ./requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from transformers==4.53.2->-r ./requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from transformers==4.53.2->-r ./requirements.txt (line 2)) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from transformers==4.53.2->-r ./requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from datasets==4.0.0->-r ./requirements.txt (line 3)) (21.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from datasets==4.0.0->-r ./requirements.txt (line 3)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from datasets==4.0.0->-r ./requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from datasets==4.0.0->-r ./requirements.txt (line 3)) (0.70.16)\n",
            "Requirement already satisfied: psutil in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from peft==0.16.0->-r ./requirements.txt (line 4)) (5.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from pandas==2.2.2->-r ./requirements.txt (line 9)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from pandas==2.2.2->-r ./requirements.txt (line 9)) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from pandas==2.2.2->-r ./requirements.txt (line 9)) (2023.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from scikit-learn==1.6.1->-r ./requirements.txt (line 10)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from scikit-learn==1.6.1->-r ./requirements.txt (line 10)) (3.6.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from tqdm==4.67.1->-r ./requirements.txt (line 12)) (0.4.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from sympy==1.13.1->torch==2.6.0+cu124->-r ./requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r ./requirements.txt (line 3)) (3.12.14)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r ./requirements.txt (line 9)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from requests->transformers==4.53.2->-r ./requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from requests->transformers==4.53.2->-r ./requirements.txt (line 2)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from requests->transformers==4.53.2->-r ./requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from requests->transformers==4.53.2->-r ./requirements.txt (line 2)) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from jinja2->torch==2.6.0+cu124->-r ./requirements.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r ./requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r ./requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r ./requirements.txt (line 3)) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r ./requirements.txt (line 3)) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r ./requirements.txt (line 3)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r ./requirements.txt (line 3)) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r ./requirements.txt (line 3)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==4.0.0->-r ./requirements.txt (line 3)) (1.20.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r ./requirements.txt \\\n",
        "  --extra-index-url https://download.pytorch.org/whl/cu124"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTmg4mHSGzMs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\kimyo\\anaconda3\\envs\\test\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from module import gemma3_seqcls_infonce  # 반드시 최상단에서 임포트!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mNtk6Mq6l6OL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
        "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer, TrainerCallback\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import datetime as dt\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1Z_SmVb3oPTK"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"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\")  # Hugging Face에서 발급받은 토큰 입력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QDJkwtDAvzxX"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "SEED = 42\n",
        "seed_everything(SEED) # Seed 고정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaU-z-Onv3Zs"
      },
      "source": [
        "# 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▶ Train folds: ['./fold1.csv', './fold2.csv', './fold3.csv']\n",
            "▶ Validation fold: ./fold0.csv\n"
          ]
        }
      ],
      "source": [
        "# 전체 fold 파일 경로 리스트 (0~4)\n",
        "val_fold_idx = 0  # 예: 3을 넣으면 fold3.csv가 validation으로, 나머지(0,1,2,4)가 train으로 사용\n",
        "\n",
        "\n",
        "fold_paths = [f\"./data/kfold_csv/fold{i}.csv\" for i in range(4)]\n",
        "\n",
        "FOLD_VAL   = fold_paths[val_fold_idx]\n",
        "FOLD_TRAIN = [path for idx, path in enumerate(fold_paths) if idx != val_fold_idx]\n",
        "\n",
        "print(\"▶ Train folds:\", FOLD_TRAIN)\n",
        "print(\"▶ Validation fold:\", FOLD_VAL)\n",
        "\n",
        "TEST_CSV        = \"./data/kfold_csv/test_preprocessed.csv\"\n",
        "SUBMISSION_CSV  = \"./data/kfold_csv/sample_submission.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk6auXNVsdsF",
        "outputId": "93672f3a-a365-4f15-8dc5-8bb11a195e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "최종 학습 샘플 수: 91143\n",
            "최종 학습 클래스 분포: {1: 45572, 0: 45571}\n",
            "검증 샘플 수: 30381\n",
            "검증 클래스 분포: {0: 15191, 1: 15190}\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 1) fold1~3 → 학습, fold0 → 검증\n",
        "# ==============================================================\n",
        "\n",
        "# 학습용 데이터프레임\n",
        "train_df = pd.concat(\n",
        "    [pd.read_csv(p, encoding=\"utf-8-sig\") for p in FOLD_TRAIN],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "# 검증용 데이터프레임\n",
        "val_df   = pd.read_csv(FOLD_VAL, encoding=\"utf-8-sig\")\n",
        "\n",
        "# ── 필요 없는 열 제거 & 컬럼명 통일 ──\n",
        "train_df = train_df[['full_text', 'generated']].rename(\n",
        "    columns={'full_text':'text', 'generated':'label'}\n",
        ")\n",
        "val_df   = val_df  [['full_text', 'generated']].rename(\n",
        "    columns={'full_text':'text', 'generated':'label'}\n",
        ")\n",
        "\n",
        "# ── 학습 세트 셔플 ──\n",
        "train_df = train_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "print(\"최종 학습 샘플 수:\", len(train_df))\n",
        "print(\"최종 학습 클래스 분포:\", train_df['label'].value_counts().to_dict())\n",
        "print(\"검증 샘플 수:\", len(val_df))\n",
        "print(\"검증 클래스 분포:\", val_df['label'].value_counts().to_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "t8qvCx-aso9K"
      },
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# 2) Hugging Face Dataset 변환\n",
        "# ==============================================================\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset   = Dataset.from_pandas(val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "fa1da093fc1f46bc837f3318b827f71e",
            "03f59eff4d744a4d934dedf8d365063e",
            "a9d15fe79d54453ba7fdd2fa113d7d84",
            "19a7644614ae4d12b83a91abe7d2aae9",
            "1f5fd6a908714106968d1237a21bcb53",
            "57d146a33e87417d8de6630802e316cb",
            "dbbb31da3f7743d790bf5e2ed506b6b7",
            "4d95116d43034fa5af59c2fde166797d",
            "84ccd0c0409540419fed1d4d07f631a3",
            "1e37e6bd70dd4424b2c25cfc2e428539",
            "fee9b15d640d48f8aee764b8f4615bb0",
            "82220b2a06a94b81a4a918e2940f8818",
            "aa1bdc768d8446cda269f72826a294c8",
            "a9f5bfc578554a1fa535a26f07e8eed1",
            "800403f529d845b68aea3b2fac416a0b",
            "1a99a7251b1b4338b296ea8ccec9584b",
            "c1869a3c39ad4e45a635c9451856bdf5",
            "6ae60d3a736b4272909a52f5835a814c",
            "94d3296632e64e64b4b24dc6dbfc0167",
            "7df2669f08cb44698d4e7530a1b53e60",
            "a9736ecbec974f2a8db54f7821f0e8b4",
            "44e3d353d8f840958aedf6f0acca4c20",
            "43f00ffd3d4546729e9bef5cf5ead542",
            "2c056d5f12ab4716bcc2f4605ccdc052",
            "cfc879f163ab4dc9b328ddaf76a6b017",
            "8d71cba9855448d49f3eb94a0c0f1bce",
            "ace25f5c883341c3a9b0c175a2dfa1d9",
            "f7c6095bc5694b9c929b5525939065c4",
            "243d8afa39a543f1afd907cc9029a4a8",
            "4b45802675834139be4e41b93f33d0a7",
            "7513a2f10a1c437385986175b16ecc71",
            "41e84b1d8bd04ccaa78de1f71b6c3cff",
            "3e6d858e5c8e47f198a48e8584466b4e",
            "62bea607f54649ad84ebd2eb86591963",
            "99518a19125a4cbaa4ed3af8b24ff309",
            "9edc664030b745078a4e2844ed645adf",
            "e0afec0635f24bdeabe10f491d7e1a85",
            "c31cc2cec47b4ad4ba472891319b6419",
            "949e256a96fd48bd8bbaa05dddbcc7af",
            "8da49247ed834504bb4975501faf67c9",
            "bd58d9108138454f82136d3fcd6f2a9f",
            "0eca4ec3c1b84007af357766cdfb5255",
            "89de8bccdb4b4ba198e13d181785b8ac",
            "e326cec5038f4120b62f62dd658d69b0",
            "fee908130e6e4a7ca5c7f4418da037b8",
            "38fd6e35e0d4440f8c5be794f9321c74",
            "a01531419938482e95354d49776796a2",
            "72a1fe7c77944de5830336e24d8c0ef6",
            "132111dc2298444ea708369e8f4892d0",
            "adf4173307c34580ad1fbe5b54305def",
            "b76470b33cd144ce8d3b76c1fa9908d9",
            "9b26c63692784609bb170c5e7d081743",
            "b880963df7c547b8b3fbe191e404850d",
            "c492d5e85307455db80bbf447f43a1c8",
            "c29cb3a8adc94f7f8d48c44b4c1be2fa",
            "7e95b396270c491092037a6774c308df",
            "e0fc29e5d0124e8289dfb0ebda4f0726",
            "a445aa43e6c445a99846ce02108290bb",
            "f5e6f6a2e06848b6a5531c3fdb15516a",
            "f968a755354f4d7594f9bdcc98150c20",
            "4cd69d47ae4a44da8bace9bd576a2c47",
            "c283eab0692445ad901aabd7ad13c641",
            "1d0c865415124f1bbcc7bfa10040c0a5",
            "4a7f12ee206942eda59dd3c8ddcdb232",
            "0e7543b7eacc4eb89fea364959125512",
            "35e5cfdd04cc4406b38a43b01f237901",
            "d3b9858317cc482fa11b8cdf5b6312ce",
            "e1a32caf05a34885bc946b8e48fff7a0",
            "fb43b805d42c457a8a04d5168248aa60",
            "b269b2838408478c844f616f902341e4",
            "6cd20f90f2df4a549303933838603a47",
            "cdf8f5bbe5a84091895247ec6d0f6e44",
            "04806817be844083ae458b665440269f",
            "f49fe0926af2403faf07184b35ccd583",
            "4b920e3aa2f3491f9ed298999bd57c94",
            "9b155761cae2439da7b47e676d6e4ba2",
            "2cefde6380994bc9ad37d9b2dda0de4d"
          ]
        },
        "id": "X27Xe0hLHggu",
        "outputId": "32bc8870-fdb4-4030-b3da-9a5435d4968c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/91143 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Map: 100%|██████████| 91143/91143 [00:09<00:00, 10078.22 examples/s]\n",
            "Map: 100%|██████████| 30381/30381 [00:03<00:00, 9089.57 examples/s] \n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 3) 토큰화\n",
        "# ==============================================================\n",
        "\n",
        "MODEL_NAME = \"google/gemma-3-12b-it\"  # 사전학습 모델 이름 (Hugging Face 모델 허브)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# 학습/검증 데이터를 토큰화\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# 토크나이저가 반환한 컬럼과 원본 텍스트 컬럼 정리 (모델 입력에 필요 없는 컬럼 제거)\n",
        "train_dataset = train_dataset.remove_columns([\"text\"])\n",
        "val_dataset = val_dataset.remove_columns([\"text\"])\n",
        "\n",
        "# 라벨 컬럼명 변경\n",
        "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
        "val_dataset   = val_dataset.rename_column(\"label\", \"labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VzDUgCrSHQi5"
      },
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# 4) Data Collator\n",
        "# ==============================================================\n",
        "data_collator = DataCollatorWithPadding(tokenizer, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "17eba53595924db49282ce5a3d254660",
            "02461e03a44f4a18bf923c2a8f0e3075",
            "9cc2fdc79fd948149ce85d025a458840",
            "b583b0be8e63437984354aa52501c697",
            "139c69704cb74c498734ef8cdf59c6fb",
            "6f3e5a563ff741dc83e6786f2000dfd6",
            "59b55d7ff51d4210bd20fe1284dab3e2",
            "b5a0be75405746aabc22a58de5deff01",
            "2c0ef28115f948a18320b96f940229ce",
            "1290be675c33436993d5dc71a5de36fa",
            "05e3ffecc3c34e6689daf503807cc966",
            "93526e8216704bc0afb6d92e2b850ddf",
            "43efd14724cb4e70a90e41434c2ef110",
            "3e467f9555f54338879e1bf2e589ef28",
            "fa7bc1bd3f4c4a099ebaf1b334058905",
            "c8bed4bda01e4d60a63abf8184dd7020",
            "75747b6f0b714a3ca599acba05b8e8c4",
            "43f0b70003594ee79471f164ffcc5b5a",
            "4e00ca699dc845f591e8a9e3a2875e5b",
            "2c605b2456bc436d85929d1a077eac0f",
            "da36e344fb5a456c98348a1d5b46fae6",
            "2ce72816a06d48269013323b3108f848",
            "8d028e3c04ef40869bf00a506df2b27b",
            "762bc67e1bde43fc858748e45396bcb8",
            "7a62fb2086ef43b7bc21d784a2964b1f",
            "20730ae1da8745a8986489f6484366c7",
            "4e03736c45ea427abd8aab51aa0caaf0",
            "69fee85c4b3d47c6b5266865dc803cfc",
            "bc06bf4510e64fecbafc16381c4627a3",
            "5b16100c98c340a88675c0bf483dcb9b",
            "965f3a89248a43cc8dcb0b0f58c0c315",
            "6639c0da31234d9097b6ece101497894",
            "7fe0f8a0ea594c64bff7d2208a9ff988",
            "1f2ad3dacc694b3983e499783fa9af98",
            "725556eb19e2472b93e28cd9669b3148",
            "a01a3261ea9b40d89de9cdb93f7266a6",
            "5d5b0031596a47a69bf7c75440aa74c9",
            "c48617d5060d4dbc93d9c01825f0b83c",
            "a1bbf07004cc464daae916f05bbc5fc4",
            "46fe5cd902c54776a322ec8ef2e17639",
            "37f1f5eac4c042f28c1dffadc875c82f",
            "9c8194f16d144911a099fbba24a10955",
            "cdf44280c2904d80a4f7fd49f781a93c",
            "f38dada708d6454cbe7519e45aa7c915",
            "fd9efb978eae444fa4a97760940c70a5",
            "a81bb3aee4f340ba90c17db423d70701",
            "01d3b538624b40ddbb7c2e9435c3fec6",
            "5c14ae7d0d094552bd586d8714b3d7f7",
            "736834f02a8148839e24e84eb0490f21",
            "4c9aed1cdfbc4110b972870406f323fb",
            "098636acec0f4bbd8d46d5051016dde5",
            "1351cd57ba6a4a0c8cd5765b50bff6b0",
            "40d8aeb4149d4ca29536662c2916c8b2",
            "24ffe5e976714ac58d14c9c2389cd135",
            "e0e08d1c9da1466394636b39b78e6c9e",
            "3597b8e53e64476dbddacae215539467",
            "df5d32a96b3b4c3ca6bae4365b48029c",
            "738c6dddd5cf4375bc981588a7bb3b6d",
            "bb2d43adc5674f10ba6f7e0b0bd69361",
            "eb1b8b0aaf72410588637e19848e1e26",
            "1103c79366c94dd4b2eac337e3065644",
            "99459611cd104fafbf6e8088374f19fa",
            "d2ace3e80be349d199b4facfde7f264a",
            "4f843e523f124200a4a20d487cc72089",
            "aa9e01063ec24ab8b7ebcb794c1071d8",
            "44a8ee481e574f2e9c4f9deb591c3901",
            "d56eaa50b4414407aa7337b1601e8d60",
            "83482dcabfe447f686ce6c5d62cf400a",
            "f34edccea2bf4302b615c739b8af3ce4",
            "2ec38cc376a14876a9bd97d591b54b24",
            "11bce19e6d8e4705b1142496d4e41c5e",
            "c2355665368b4a36a4c1a87edfafc038",
            "539da5261cdd455fbf900f9e6d725648",
            "ade40769f5254379a73e398c22585a0a",
            "cea8cb6d6b7540a1ae58aa618855aed6",
            "7ae2b068fd054609b39c657911b4af7f",
            "6b82fd7e963e49a7ad5c7bd452468bde",
            "db3f90e8be27457db74748e3cb849454",
            "8ee30b3af0ee42e6a0c35680fb7b733a",
            "6b4aacb276274ea691a00b2d13c875cd",
            "b4a5738be4374f7fa9ec2ff1ebe99136",
            "5c5ce9a31ff1435da89a0f9ce51c7c58",
            "024c248fd9af4d66a48cd15160114ca5",
            "af3dc04b147748c7984da7cc6df0fe7d",
            "49a1badf09324976b9d330bdcd189dfc",
            "d1d47dac33b849899bb579ee21338f96",
            "ecc34f6ffe424933bb12590ed4f35da6",
            "a9915ba4cf5a4da48e1e4e7ee5e9ea2c",
            "1d49bdb48dca431c946a0b1b44cc7700",
            "4c9d630641fc416e9e7ef866c9aa75a7",
            "be712bfc8c80421599af745a4f9b9b4c",
            "6bd32d895d244718845cf0d359e87c11",
            "d7747405aaac4cdfab5b712e3b6f49ee",
            "456c5040d1864f0da2208a01a426ee5c",
            "1b03da303ec54616b826aba26c8852b4",
            "39e8f1f7e1844e9e8502de5672127fad",
            "9f724e1e50de4c898349af8e1af226e4",
            "22df781c4f794f9ab2b424cf34fc7bc7",
            "c57ef8c02b614e1885f7dbd0862b47f4"
          ]
        },
        "id": "HuD8Qn_sZfB4",
        "outputId": "917b39a2-2b60-4a12-ad3b-cfeadba98c08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 5 files: 100%|██████████| 5/5 [00:50<00:00, 10.05s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 5/5 [00:17<00:00,  3.44s/it]\n",
            "Some weights of Gemma3ForSequenceClassification were not initialized from the model checkpoint at google/gemma-3-12b-it and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Gemma3ForSequenceClassification(\n",
              "  (vision_tower): SiglipVisionModel(\n",
              "    (vision_model): SiglipVisionTransformer(\n",
              "      (embeddings): SiglipVisionEmbeddings(\n",
              "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
              "        (position_embedding): Embedding(4096, 1152)\n",
              "      )\n",
              "      (encoder): SiglipEncoder(\n",
              "        (layers): ModuleList(\n",
              "          (0-26): 27 x SiglipEncoderLayer(\n",
              "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attn): SiglipAttention(\n",
              "              (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "              (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "              (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "              (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "            )\n",
              "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): SiglipMLP(\n",
              "              (activation_fn): PytorchGELUTanh()\n",
              "              (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "              (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (multi_modal_projector): Gemma3MultiModalProjector(\n",
              "    (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
              "    (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
              "  )\n",
              "  (language_model): Gemma3TextModel(\n",
              "    (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 3840, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-47): 48 x Gemma3DecoderLayer(\n",
              "        (self_attn): Gemma3Attention(\n",
              "          (q_proj): Linear4bit(in_features=3840, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=3840, out_features=2048, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=3840, out_features=2048, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=3840, bias=False)\n",
              "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "        )\n",
              "        (mlp): Gemma3MLP(\n",
              "          (gate_proj): Linear4bit(in_features=3840, out_features=15360, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=3840, out_features=15360, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=15360, out_features=3840, bias=False)\n",
              "          (act_fn): PytorchGELUTanh()\n",
              "        )\n",
              "        (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
              "        (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
              "        (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
              "        (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
              "    (rotary_emb): Gemma3RotaryEmbedding()\n",
              "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
              "  )\n",
              "  (score): Linear(in_features=3840, out_features=2, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 장치 설정 (GPU 사용 가능 여부)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # A100에서는 bfloat16 사용 권장\n",
        "    bnb_4bit_quant_type=\"nf4\",             # NF4 양자화 방식\n",
        "    bnb_4bit_use_double_quant=True         # 메모리 효율 추가 향상 옵션\n",
        ")\n",
        "\n",
        "# 사전훈련 모델 로드 (시퀀스 분류용 헤드 포함) 및 GPU 이동\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2, quantization_config=bnb_config, torch_dtype=torch.bfloat16)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4iutaayDH5Tl"
      },
      "outputs": [],
      "source": [
        "# LoRA 설정 구성\n",
        "R = 32\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.1\n",
        "lora_config = LoraConfig(\n",
        "    r=R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    target_modules= [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        ")\n",
        "\n",
        "# 원본 모델에 LoRA 어댑터 추가\n",
        "model = get_peft_model(model, lora_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvBYhxtsawF-",
        "outputId": "4393b5c9-f560-431a-f3a8-d2c61abfb781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 136,920,576 || all params: 12,324,253,296 || trainable%: 1.1110\n"
          ]
        }
      ],
      "source": [
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ENRRI1zmbKjy"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    probs = logits[:, 1]  # 클래스 1의 확률 추정값\n",
        "    roc_auc = roc_auc_score(labels, probs)\n",
        "    return {\"roc_auc\": roc_auc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuXgli-npscW"
      },
      "outputs": [],
      "source": [
        "class ScheduledCLTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    1 에폭 동안\n",
        "    - 처음 delay_ratio 비율만큼은 lambda_cl=0\n",
        "    - 이후 에폭 종료까지 선형적으로 max_lambda 까지 올림\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, delay_ratio: float = 0.3, max_lambda: float = 0.05, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.delay_ratio = delay_ratio\n",
        "        self.max_lambda  = max_lambda\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs[\"labels\"]\n",
        "        step   = self.state.global_step        # 현재 스텝 (0부터 시작)\n",
        "        total  = self.state.max_steps          # 1 에폭 전체 스텝 수\n",
        "\n",
        "        # ── delay 구간 스텝 계산 ─────────────────────\n",
        "        delay_steps = int(total * self.delay_ratio)\n",
        "\n",
        "        # ── lambda_cl 계산 ───────────────────────────\n",
        "        if step < delay_steps:\n",
        "            lambda_cl = 0.0\n",
        "        else:\n",
        "            # 남은 구간을 0→1 로 노말라이즈\n",
        "            rem_steps = total - delay_steps\n",
        "            rel_step  = step - delay_steps\n",
        "            progress  = min(rel_step / rem_steps, 1.0)\n",
        "            lambda_cl = progress * self.max_lambda\n",
        "\n",
        "        # ── forward 호출 ─────────────────────────────\n",
        "        outputs = model(\n",
        "            **inputs,\n",
        "            contrastive_labels=labels,\n",
        "            lambda_cl=lambda_cl,\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-e9p_5QbTT_"
      },
      "outputs": [],
      "source": [
        "# 훈련 파라미터 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./train&inference/gemma/fold0/gemma_model0_checkpoint\",\n",
        "    overwrite_output_dir=True,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    metric_for_best_model=\"roc_auc\",\n",
        "    greater_is_better=True,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1000,\n",
        "    logging_first_step=True,\n",
        "    save_total_limit=2,\n",
        "    seed=SEED,\n",
        "    dataloader_drop_last=False,\n",
        "    report_to=\"none\",\n",
        "    label_names=[\"labels\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUpU4v0BcekZ",
        "outputId": "c29c373e-1b68-4921-c02c-3f079f10f814"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_138/2341002103.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `ScheduledCLTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Trainer 객체 생성\n",
        "trainer = ScheduledCLTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    delay_ratio=0.3,    # 에폭의 30% 동안 CL 꺼둠\n",
        "    max_lambda=0.05,    # 이후 선형 상승하여 최종 0.05\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "CeK2a_QXbw9W",
        "outputId": "155ffab7-5a3e-4a44-8c44-a04dbfaf949c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6723' max='11393' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 6723/11393 1:53:13 < 1:18:40, 0.99 it/s, Epoch 0.59/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.668500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.553000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.533500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.529700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.534300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.548300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub message rate exceeded.\n",
            "The Jupyter server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--ServerApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "ServerApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 모델 훈련 시작\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMq8b72ffU4B",
        "outputId": "cf2514ac-d8e7-4840-b91d-37896aaad0d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모델이 저장되었습니다: ./gemma_model0\n"
          ]
        }
      ],
      "source": [
        "# fine-tuned 모델을 로컬에 저장\n",
        "output_dir = \"./train&inference/gemma/fold0/gemma_model0\"\n",
        "trainer.model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(\"모델이 저장되었습니다:\", output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TEST 데이터셋 추론"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "테스트 샘플 수: 1962\n"
          ]
        }
      ],
      "source": [
        "# 테스트 데이터 불러오기\n",
        "test_df = pd.read_csv(TEST_CSV, encoding='utf-8-sig')\n",
        "submission_df = pd.read_csv(SUBMISSION_CSV, encoding='utf-8-sig')\n",
        "\n",
        "print(\"테스트 샘플 수:\", len(test_df))\n",
        "# 각 테스트 샘플에 대해 추론\n",
        "pred_probs = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForSequenceClassification(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Gemma3ForSequenceClassification(\n",
              "      (vision_tower): SiglipVisionModel(\n",
              "        (vision_model): SiglipVisionTransformer(\n",
              "          (embeddings): SiglipVisionEmbeddings(\n",
              "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
              "            (position_embedding): Embedding(4096, 1152)\n",
              "          )\n",
              "          (encoder): SiglipEncoder(\n",
              "            (layers): ModuleList(\n",
              "              (0-26): 27 x SiglipEncoderLayer(\n",
              "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "                (self_attn): SiglipAttention(\n",
              "                  (k_proj): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (v_proj): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (q_proj): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=1152, out_features=32, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=32, out_features=1152, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "                )\n",
              "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "                (mlp): SiglipMLP(\n",
              "                  (activation_fn): PytorchGELUTanh()\n",
              "                  (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "                  (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (multi_modal_projector): Gemma3MultiModalProjector(\n",
              "        (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
              "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
              "      )\n",
              "      (language_model): Gemma3TextModel(\n",
              "        (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 3840, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-47): 48 x Gemma3DecoderLayer(\n",
              "            (self_attn): Gemma3Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3840, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3840, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3840, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3840, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3840, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3840, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=3840, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3840, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "            )\n",
              "            (mlp): Gemma3MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3840, out_features=15360, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3840, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=15360, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3840, out_features=15360, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3840, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=15360, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=15360, out_features=3840, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=15360, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=3840, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): PytorchGELUTanh()\n",
              "            )\n",
              "            (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
              "            (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
              "            (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
              "            (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
              "        (rotary_emb): Gemma3RotaryEmbedding()\n",
              "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
              "      )\n",
              "      (score): ModulesToSaveWrapper(\n",
              "        (original_module): Linear(in_features=3840, out_features=2, bias=False)\n",
              "        (modules_to_save): ModuleDict(\n",
              "          (default): Linear(in_features=3840, out_features=2, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 추론 파이프라인 구성 (GPU 사용, 모든 클래스 점수 출력)\n",
        "clf = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=trainer.model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_all_scores=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "샘플 결과 예시: [[{'label': 'LABEL_0', 'score': 0.6584175229072571}, {'label': 'LABEL_1', 'score': 0.3415825068950653}]]\n"
          ]
        }
      ],
      "source": [
        "print(\"샘플 결과 예시:\", clf(test_df['paragraph_text'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        }
      ],
      "source": [
        "for text in test_df['paragraph_text']:\n",
        "    scores = clf(text)[0]\n",
        "    prob_ai = None\n",
        "    for s in scores:\n",
        "        if s['label'] in ['LABEL_1', '1', 'generated']:\n",
        "            prob_ai = s['score']\n",
        "            break\n",
        "    if prob_ai is None:\n",
        "        prob_ai = scores[1]['score']\n",
        "    pred_probs.append(prob_ai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 결과를 제출 데이터프레임에 기록\n",
        "submission_df['generated'] = pred_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TEST_0000</td>\n",
              "      <td>0.341583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TEST_0001</td>\n",
              "      <td>0.414899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TEST_0002</td>\n",
              "      <td>0.222700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TEST_0003</td>\n",
              "      <td>0.819893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TEST_0004</td>\n",
              "      <td>0.893309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1957</th>\n",
              "      <td>TEST_1957</td>\n",
              "      <td>0.991084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1958</th>\n",
              "      <td>TEST_1958</td>\n",
              "      <td>0.992000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1959</th>\n",
              "      <td>TEST_1959</td>\n",
              "      <td>0.317426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1960</th>\n",
              "      <td>TEST_1960</td>\n",
              "      <td>0.250913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1961</th>\n",
              "      <td>TEST_1961</td>\n",
              "      <td>0.256832</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1962 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             ID  generated\n",
              "0     TEST_0000   0.341583\n",
              "1     TEST_0001   0.414899\n",
              "2     TEST_0002   0.222700\n",
              "3     TEST_0003   0.819893\n",
              "4     TEST_0004   0.893309\n",
              "...         ...        ...\n",
              "1957  TEST_1957   0.991084\n",
              "1958  TEST_1958   0.992000\n",
              "1959  TEST_1959   0.317426\n",
              "1960  TEST_1960   0.250913\n",
              "1961  TEST_1961   0.256832\n",
              "\n",
              "[1962 rows x 2 columns]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "submission_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission_df.to_csv(\"./ensemble/data/test_ensemble_folding/test_gemma_fold0.csv\", index=False, encoding=\"utf-8-sig\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VAL 데이터셋 배치 추론"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "AgJjDWE0MaQ4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 30381/30381 [00:02<00:00, 11528.74 examples/s]\n"
          ]
        }
      ],
      "source": [
        "def tokenize_test(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True)\n",
        "\n",
        "val_ds = Dataset.from_pandas(val_df)\n",
        "\n",
        "val_ds = val_ds.map(tokenize_test, batched=True,\n",
        "                      remove_columns=[\"text\", \"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate(features):\n",
        "    \"\"\"\n",
        "    • 동적 padding → tensor 변환\n",
        "    • tokenizer가 추가한 'length' 류 메타키 제거\n",
        "    \"\"\"\n",
        "    batch = data_collator(features)\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 79%|███████▉  | 2999/3798 [14:59<04:12,  3.16it/s]IOPub message rate exceeded.\n",
            "The Jupyter server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--ServerApp.iopub_msg_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
            "ServerApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "BATCH_TEST = 8\n",
        "loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_TEST,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "probs_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(loader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        logits = trainer.model(**batch).logits\n",
        "        probs  = torch.softmax(logits, dim=-1)[:, 1]\n",
        "        probs_list.append(probs.cpu())\n",
        "\n",
        "probs = torch.cat(probs_list).to(torch.float32).numpy()\n",
        "print(f\"[✓] Inference done – {len(probs)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_df['generated'] = probs\n",
        "val_df['ID'] = pd.read_csv(FOLD_VAL, encoding=\"utf-8-sig\")['id']\n",
        "val_df = val_df[['ID', 'generated', 'label']]\n",
        "val_df.to_csv(\"./ensemble/data/val_ensemble_folding/val_gemma_fold0.csv\", index=False, encoding=\"utf-8-sig\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
